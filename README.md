# A Comprehensive Survey on Hyperparameter Tuning, Regularization, and Optimization in Deep Neural Networks
Welcome to the official repository for the research survey paper:

**Live Paper (Hosted via GitHub Pages):**  
üëâ [https://hartz-byte.github.io/survey-deep-hpo/](https://hartz-byte.github.io/survey-deep-hpo/)

---

## Overview
This paper presents an in-depth exploration of hyperparameters in deep neural networks, focusing on:

- Hyperparameter definitions and types  
- Regularization methods (L1, L2, dropout, etc.)  
- Optimization strategies (SGD, Adam, RMSProp, etc.)  
- Tuning techniques (Grid Search, Bayesian Optimization, AutoML)  
- Best practices (learning curves, robustness, experiment tracking)

With detailed explanations, mathematical formulations, and visual illustrations, this survey acts as a complete guide for both researchers and practitioners working with deep learning.

---

## Topics Covered
- Foundations of Hyperparameters  
- Categories: Architecture, Training, Regularization, Optimization, EWA  
- Hyperparameter Tuning Strategies  
- Regularization Techniques (Dropout, Batch Norm, Label Smoothing, etc.)  
- Optimization Algorithms (SGD, Adam, Learning Rate Decay, etc.)  
- Best Practices: Learning Curves, Robustness, Experiment Tracking  
- Challenges in Tuning and Model Generalization  
- Conclusion, Recommendations, and Acknowledgements

---

## ‚≠êÔ∏è If you find this paper helpful...
Please give it a star! ‚≠êÔ∏è

Your support encourages me to keep learning, writing, and sharing knowledge with the community.

---

## Author
Harsh Gupta

Independent Researcher

üì´ harshmail281199@gmail.com

---
